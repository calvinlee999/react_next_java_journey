# Azure Databricks Integration for Modern Big Data Platform

## Overview

This comprehensive Azure Databricks integration provides unified real-time, near-time, and batch processing capabilities with Delta Lake storage, MLflow model management, and seamless integration with our event-driven architecture for scalable AI inference.

## ðŸ—ï¸ Databricks Architecture

### Unified Processing Platform

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            AZURE DATABRICKS UNIFIED PLATFORM                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Real-Time     â”‚  â”‚   Near-Time     â”‚  â”‚     Batch       â”‚  â”‚   ML Training   â”‚ â”‚
â”‚  â”‚   Processing    â”‚  â”‚   Processing    â”‚  â”‚   Processing    â”‚  â”‚   & Inference   â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚ â”‚
â”‚  â”‚ â€¢ Structured    â”‚  â”‚ â€¢ Micro-batch   â”‚  â”‚ â€¢ ETL Jobs      â”‚  â”‚ â€¢ AutoML        â”‚ â”‚
â”‚  â”‚   Streaming     â”‚  â”‚ â€¢ 5min windows  â”‚  â”‚ â€¢ Data Prep     â”‚  â”‚ â€¢ MLflow        â”‚ â”‚
â”‚  â”‚ â€¢ Kafka Direct â”‚  â”‚ â€¢ Tumbling      â”‚  â”‚ â€¢ Aggregations  â”‚  â”‚ â€¢ Model Serving â”‚ â”‚
â”‚  â”‚ â€¢ Delta Live    â”‚  â”‚ â€¢ Watermarks    â”‚  â”‚ â€¢ OLAP Cubes    â”‚  â”‚ â€¢ A/B Testing   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                       â”‚                       â”‚                       â”‚  â”‚
â”‚           â”‚                       â”‚                       â”‚                       â”‚  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                            DELTA LAKE STORAGE LAYER                             â”‚ â”‚
â”‚  â”‚                                                                                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚ â”‚
â”‚  â”‚  â”‚   Bronze    â”‚  â”‚   Silver    â”‚  â”‚    Gold     â”‚  â”‚  Feature    â”‚            â”‚ â”‚
â”‚  â”‚  â”‚    Layer    â”‚  â”‚    Layer    â”‚  â”‚   Layer     â”‚  â”‚   Store     â”‚            â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚            â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Raw Data  â”‚â”€â”€â–¶â”‚ â€¢ Cleaned   â”‚â”€â”€â–¶â”‚ â€¢ Business â”‚  â”‚ â€¢ ML Ready  â”‚            â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Schema    â”‚  â”‚ â€¢ Validated â”‚  â”‚   Logic     â”‚  â”‚ â€¢ Features  â”‚            â”‚ â”‚
â”‚  â”‚  â”‚   Evolution â”‚  â”‚ â€¢ Quality   â”‚  â”‚ â€¢ Aggregatedâ”‚  â”‚ â€¢ Versioned â”‚            â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Audit     â”‚  â”‚   Metrics   â”‚  â”‚ â€¢ Curated   â”‚  â”‚ â€¢ Governed  â”‚            â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Real-Time Processing with Structured Streaming

### Kafka to Delta Lake Real-Time Pipeline

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import DeltaTable
import json
from typing import Dict, Any, List

class DatabricksRealTimeProcessor:
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.checkpoint_location = "/mnt/checkpoints/realtime"
        
    def create_kafka_stream(self, 
                           kafka_servers: str,
                           topic: str,
                           schema: StructType,
                           starting_offsets: str = "latest") -> DataFrame:
        """Create real-time stream from Kafka"""
        
        kafka_df = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_servers) \
            .option("subscribe", topic) \
            .option("startingOffsets", starting_offsets) \
            .option("maxOffsetsPerTrigger", 10000) \
            .option("kafka.session.timeout.ms", "30000") \
            .option("kafka.request.timeout.ms", "40000") \
            .load()
        
        # Parse JSON and apply schema
        parsed_df = kafka_df \
            .select(
                from_json(col("value").cast("string"), schema).alias("data"),
                col("key").cast("string").alias("message_key"),
                col("topic"),
                col("partition"),
                col("offset"),
                col("timestamp").alias("kafka_timestamp")
            ) \
            .select("data.*", "message_key", "topic", "partition", "offset", "kafka_timestamp")
        
        return parsed_df
    
    def apply_governance_and_quality(self, df: DataFrame) -> DataFrame:
        """Apply real-time governance and quality checks"""
        
        # Add governance metadata
        governed_df = df \
            .withColumn("governance_timestamp", current_timestamp()) \
            .withColumn("governance_session_id", lit(uuid.uuid4().hex)) \
            .withColumn("data_classification", 
                       when(col("amount") > 10000, "HIGH_VALUE")
                       .when(col("amount") > 1000, "MEDIUM_VALUE")
                       .otherwise("LOW_VALUE")) \
            .withColumn("pii_detected", 
                       when(col("customer_email").isNotNull() | 
                            col("customer_phone").isNotNull(), True)
                       .otherwise(False)) \
            .withColumn("requires_encryption", 
                       when(col("pii_detected") | (col("amount") > 5000), True)
                       .otherwise(False))
        
        # Add quality metrics
        quality_df = governed_df \
            .withColumn("quality_completeness", 
                       (size(array_remove(array(*[col(c) for c in df.columns]), None)) / lit(len(df.columns)))) \
            .withColumn("quality_validity",
                       when((col("amount") > 0) & 
                            col("customer_id").rlike("^[A-Z0-9]{8,12}$") &
                            col("transaction_date").isNotNull(), 1.0)
                       .otherwise(0.0)) \
            .withColumn("quality_score", 
                       (col("quality_completeness") + col("quality_validity")) / 2)
        
        # Filter out low quality records
        filtered_df = quality_df.filter(col("quality_score") >= 0.7)
        
        return filtered_df
    
    def write_to_delta_bronze(self, df: DataFrame, table_name: str) -> StreamingQuery:
        """Write to Bronze layer with exactly-once semantics"""
        
        # Add Delta Lake metadata
        delta_df = df \
            .withColumn("_bronze_timestamp", current_timestamp()) \
            .withColumn("_bronze_date", current_date()) \
            .withColumn("_source_file", lit("kafka_stream")) \
            .withColumn("_batch_id", expr("uuid()"))
        
        # Write to Delta Lake with exactly-once processing
        query = delta_df \
            .writeStream \
            .format("delta") \
            .outputMode("append") \
            .option("checkpointLocation", f"{self.checkpoint_location}/{table_name}") \
            .option("mergeSchema", "true") \
            .partitionBy("_bronze_date") \
            .trigger(processingTime="10 seconds") \
            .table(f"bronze.{table_name}")
        
        return query
    
    def process_bronze_to_silver(self, 
                                bronze_table: str, 
                                silver_table: str,
                                business_rules: Dict[str, Any]) -> StreamingQuery:
        """Process Bronze to Silver with business logic"""
        
        # Read from Bronze as stream
        bronze_df = self.spark \
            .readStream \
            .format("delta") \
            .table(f"bronze.{bronze_table}")
        
        # Apply business transformations
        silver_df = bronze_df \
            .withColumn("transaction_amount_usd", 
                       when(col("currency") == "EUR", col("amount") * 1.1)
                       .when(col("currency") == "GBP", col("amount") * 1.25)
                       .otherwise(col("amount"))) \
            .withColumn("risk_category",
                       when(col("transaction_amount_usd") > 50000, "HIGH_RISK")
                       .when(col("transaction_amount_usd") > 10000, "MEDIUM_RISK")
                       .otherwise("LOW_RISK")) \
            .withColumn("fraud_score", 
                       when(col("merchant_category") == "ATM", 0.1)
                       .when(col("transaction_amount_usd") > col("avg_daily_amount") * 5, 0.8)
                       .when(hour(col("transaction_date")).between(2, 5), 0.3)
                       .otherwise(0.1)) \
            .withColumn("compliance_flags",
                       array(
                           when(col("transaction_amount_usd") > 10000, "REPORT_CTR").otherwise(None),
                           when(col("risk_category") == "HIGH_RISK", "MANUAL_REVIEW").otherwise(None),
                           when(col("fraud_score") > 0.7, "FRAUD_ALERT").otherwise(None)
                       )) \
            .withColumn("_silver_timestamp", current_timestamp()) \
            .withColumn("_silver_date", current_date())
        
        # Write to Silver layer
        query = silver_df \
            .writeStream \
            .format("delta") \
            .outputMode("append") \
            .option("checkpointLocation", f"{self.checkpoint_location}/silver_{silver_table}") \
            .partitionBy("_silver_date", "risk_category") \
            .trigger(processingTime="30 seconds") \
            .table(f"silver.{silver_table}")
        
        return query
    
    def create_gold_aggregations(self, 
                               silver_table: str, 
                               gold_table: str) -> StreamingQuery:
        """Create Gold layer aggregations for analytics"""
        
        # Read Silver stream
        silver_df = self.spark \
            .readStream \
            .format("delta") \
            .table(f"silver.{silver_table}")
        
        # Windowed aggregations
        windowed_df = silver_df \
            .withWatermark("transaction_date", "10 minutes") \
            .groupBy(
                window(col("transaction_date"), "5 minutes", "1 minute"),
                col("risk_category"),
                col("merchant_category")
            ) \
            .agg(
                count("*").alias("transaction_count"),
                sum("transaction_amount_usd").alias("total_amount"),
                avg("transaction_amount_usd").alias("avg_amount"),
                avg("fraud_score").alias("avg_fraud_score"),
                countDistinct("customer_id").alias("unique_customers"),
                max("transaction_amount_usd").alias("max_amount"),
                min("transaction_amount_usd").alias("min_amount")
            ) \
            .withColumn("window_start", col("window.start")) \
            .withColumn("window_end", col("window.end")) \
            .withColumn("_gold_timestamp", current_timestamp()) \
            .drop("window")
        
        # Write to Gold layer using merge for updates
        def merge_to_gold(batch_df, batch_id):
            # Create or get Delta table
            try:
                gold_delta = DeltaTable.forName(self.spark, f"gold.{gold_table}")
            except:
                # Table doesn't exist, create it
                batch_df.write.format("delta").mode("overwrite").saveAsTable(f"gold.{gold_table}")
                return
            
            # Merge logic for upserts
            gold_delta.alias("target") \
                .merge(
                    batch_df.alias("source"),
                    "target.window_start = source.window_start AND " +
                    "target.risk_category = source.risk_category AND " +
                    "target.merchant_category = source.merchant_category"
                ) \
                .whenMatchedUpdateAll() \
                .whenNotMatchedInsertAll() \
                .execute()
        
        query = windowed_df \
            .writeStream \
            .foreachBatch(merge_to_gold) \
            .option("checkpointLocation", f"{self.checkpoint_location}/gold_{gold_table}") \
            .trigger(processingTime="1 minute") \
            .start()
        
        return query

# Financial Transaction Schema
financial_transaction_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("merchant_id", StringType(), True),
    StructField("merchant_category", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("currency", StringType(), True),
    StructField("transaction_date", TimestampType(), True),
    StructField("customer_email", StringType(), True),
    StructField("customer_phone", StringType(), True),
    StructField("avg_daily_amount", DoubleType(), True),
    StructField("account_balance", DoubleType(), True),
    StructField("location_country", StringType(), True),
    StructField("location_city", StringType(), True)
])
```

## ðŸ”„ Near-Time Processing with Micro-Batching

### Optimized Micro-Batch Processing

```scala
package com.fintech.databricks.neartime

import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}
import io.delta.tables.DeltaTable

case class ProcessingWindow(
  windowSize: String,
  slideInterval: String,
  watermarkDelay: String
)

case class AggregationRule(
  groupByColumns: Seq[String],
  aggregations: Map[String, String],
  outputTable: String
)

class NearTimeProcessor(spark: SparkSession) {
  
  import spark.implicits._
  
  private val checkpointPath = "/mnt/checkpoints/neartime"
  
  def processRiskAssessment(
    sourceTable: String,
    targetTable: String,
    window: ProcessingWindow
  ): StreamingQuery = {
    
    val sourceDF = spark
      .readStream
      .format("delta")
      .table(sourceTable)
      .withWatermark("transaction_date", window.watermarkDelay)
    
    // Complex risk assessment with sliding windows
    val riskAssessmentDF = sourceDF
      .groupBy(
        window(col("transaction_date"), window.windowSize, window.slideInterval),
        col("customer_id"),
        col("merchant_category")
      )
      .agg(
        count("*").alias("transaction_count"),
        sum("amount").alias("total_amount"),
        avg("amount").alias("avg_amount"),
        stddev("amount").alias("amount_stddev"),
        countDistinct("merchant_id").alias("unique_merchants"),
        collect_list("location_city").alias("cities_visited"),
        max("transaction_date").alias("last_transaction"),
        min("transaction_date").alias("first_transaction")
      )
      .withColumn("velocity_score",
        when(col("transaction_count") > 10, 0.8)
        .when(col("transaction_count") > 5, 0.5)
        .otherwise(0.2))
      .withColumn("amount_anomaly_score",
        when(col("amount_stddev") > col("avg_amount") * 2, 0.9)
        .when(col("amount_stddev") > col("avg_amount"), 0.6)
        .otherwise(0.1))
      .withColumn("geography_score",
        when(size(array_distinct(col("cities_visited"))) > 3, 0.7)
        .when(size(array_distinct(col("cities_visited"))) > 1, 0.4)
        .otherwise(0.1))
      .withColumn("composite_risk_score",
        (col("velocity_score") + col("amount_anomaly_score") + col("geography_score")) / 3)
      .withColumn("risk_level",
        when(col("composite_risk_score") > 0.7, "HIGH")
        .when(col("composite_risk_score") > 0.4, "MEDIUM")
        .otherwise("LOW"))
      .withColumn("processing_timestamp", current_timestamp())
      .withColumn("window_start", col("window.start"))
      .withColumn("window_end", col("window.end"))
      .drop("window")
    
    // Write with micro-batch processing
    riskAssessmentDF
      .writeStream
      .format("delta")
      .outputMode("append")
      .option("checkpointLocation", s"$checkpointPath/risk_assessment")
      .partitionBy("risk_level")
      .trigger(Trigger.ProcessingTime(window.slideInterval))
      .table(targetTable)
  }
  
  def processComplianceMonitoring(
    sourceTable: String,
    targetTable: String
  ): StreamingQuery = {
    
    val sourceDF = spark
      .readStream
      .format("delta")
      .table(sourceTable)
      .withWatermark("transaction_date", "15 minutes")
    
    // Compliance pattern detection
    val complianceDF = sourceDF
      .groupBy(
        window(col("transaction_date"), "10 minutes", "5 minutes"),
        col("customer_id")
      )
      .agg(
        sum("amount").alias("period_total"),
        count("*").alias("transaction_count"),
        collect_list(struct("amount", "merchant_category", "transaction_date")).alias("transactions")
      )
      .withColumn("suspicious_patterns",
        array(
          // Structuring detection (multiple transactions just under reporting threshold)
          when(
            size(filter(col("transactions"), 
              $"col.amount".between(9000, 9999))) >= 3, 
            "POTENTIAL_STRUCTURING"
          ).otherwise(null),
          
          // Rapid fire transactions
          when(col("transaction_count") > 20, "RAPID_FIRE").otherwise(null),
          
          // High velocity cash transactions
          when(
            col("period_total") > 50000 && 
            size(filter(col("transactions"), $"col.merchant_category" === "ATM")) > 5,
            "HIGH_VELOCITY_CASH"
          ).otherwise(null),
          
          // Cross-border pattern
          when(
            size(array_distinct(transform(col("transactions"), 
              x => x.getField("merchant_category")))) > 10,
            "DIVERSE_MERCHANT_PATTERN"
          ).otherwise(null)
        ))
      .withColumn("compliance_alert_level",
        when(size(array_remove(col("suspicious_patterns"), null)) > 2, "CRITICAL")
        .when(size(array_remove(col("suspicious_patterns"), null)) > 0, "WARNING")
        .otherwise("NORMAL"))
      .withColumn("requires_sar_filing",
        col("period_total") > 10000 && 
        col("compliance_alert_level").isin("CRITICAL", "WARNING"))
      .withColumn("processing_timestamp", current_timestamp())
      .withColumn("window_start", col("window.start"))
      .withColumn("window_end", col("window.end"))
      .drop("window")
      .filter(col("compliance_alert_level") =!= "NORMAL")
    
    complianceDF
      .writeStream
      .format("delta")
      .outputMode("append")
      .option("checkpointLocation", s"$checkpointPath/compliance_monitoring")
      .partitionBy("compliance_alert_level")
      .trigger(Trigger.ProcessingTime("5 minutes"))
      .table(targetTable)
  }
  
  def processCustomerBehaviorAnalytics(
    sourceTable: String,
    targetTable: String
  ): StreamingQuery = {
    
    val sourceDF = spark
      .readStream
      .format("delta")
      .table(sourceTable)
      .withWatermark("transaction_date", "20 minutes")
    
    // Customer behavior analytics with session windows
    val behaviorDF = sourceDF
      .groupBy(
        session_window(col("transaction_date"), "30 minutes"),
        col("customer_id")
      )
      .agg(
        count("*").alias("session_transaction_count"),
        sum("amount").alias("session_total"),
        avg("amount").alias("session_avg_amount"),
        collect_list("merchant_category").alias("merchant_categories"),
        collect_list("location_city").alias("session_cities"),
        min("transaction_date").alias("session_start"),
        max("transaction_date").alias("session_end")
      )
      .withColumn("session_duration_minutes",
        (unix_timestamp(col("session_end")) - unix_timestamp(col("session_start"))) / 60)
      .withColumn("transaction_frequency",
        col("session_transaction_count") / col("session_duration_minutes"))
      .withColumn("merchant_diversity",
        size(array_distinct(col("merchant_categories"))))
      .withColumn("geographic_spread",
        size(array_distinct(col("session_cities"))))
      .withColumn("behavior_score",
        least(
          col("transaction_frequency") / 10.0, // Normalize to 0-1
          col("merchant_diversity") / 20.0,
          col("geographic_spread") / 10.0
        ))
      .withColumn("behavior_classification",
        when(col("behavior_score") > 0.8, "HIGHLY_ACTIVE")
        .when(col("behavior_score") > 0.5, "MODERATELY_ACTIVE")
        .when(col("behavior_score") > 0.2, "LOW_ACTIVITY")
        .otherwise("MINIMAL_ACTIVITY"))
      .withColumn("processing_timestamp", current_timestamp())
      .withColumn("session_window_start", col("session_window.start"))
      .withColumn("session_window_end", col("session_window.end"))
      .drop("session_window")
    
    behaviorDF
      .writeStream
      .format("delta")
      .outputMode("append")
      .option("checkpointLocation", s"$checkpointPath/customer_behavior")
      .partitionBy("behavior_classification")
      .trigger(Trigger.ProcessingTime("10 minutes"))
      .table(targetTable)
  }
}

// Usage Example
object NearTimeProcessorApp {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("NearTimeProcessor")
      .config("spark.sql.adaptive.enabled", "true")
      .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
      .config("spark.databricks.delta.optimizeWrite.enabled", "true")
      .config("spark.databricks.delta.autoCompact.enabled", "true")
      .getOrCreate()
    
    val processor = new NearTimeProcessor(spark)
    
    // Risk assessment with 5-minute windows
    val riskWindow = ProcessingWindow("5 minutes", "1 minute", "10 minutes")
    val riskQuery = processor.processRiskAssessment(
      "silver.financial_transactions",
      "gold.risk_assessment",
      riskWindow
    )
    
    // Compliance monitoring
    val complianceQuery = processor.processComplianceMonitoring(
      "silver.financial_transactions",
      "gold.compliance_alerts"
    )
    
    // Customer behavior analytics
    val behaviorQuery = processor.processCustomerBehaviorAnalytics(
      "silver.financial_transactions",
      "gold.customer_behavior"
    )
    
    // Monitor queries
    List(riskQuery, complianceQuery, behaviorQuery).foreach { query =>
      query.awaitTermination()
    }
  }
}
```

## ðŸ“Š Batch Processing with Delta Lake Optimization

### High-Performance ETL Jobs

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import DeltaTable
from typing import Dict, List, Any
import delta

class DeltaLakeBatchProcessor:
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.spark.conf.set("spark.sql.adaptive.enabled", "true")
        self.spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
        self.spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
        self.spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
    
    def create_daily_aggregations(self, 
                                 source_table: str, 
                                 target_table: str, 
                                 processing_date: str) -> None:
        """Create comprehensive daily aggregations"""
        
        # Read Silver data for specific date
        daily_transactions = self.spark.read \
            .format("delta") \
            .table(source_table) \
            .filter(col("_silver_date") == processing_date)
        
        # Customer aggregations
        customer_daily = daily_transactions \
            .groupBy("customer_id", "_silver_date") \
            .agg(
                count("*").alias("daily_transaction_count"),
                sum("transaction_amount_usd").alias("daily_total_amount"),
                avg("transaction_amount_usd").alias("daily_avg_amount"),
                max("transaction_amount_usd").alias("daily_max_amount"),
                countDistinct("merchant_id").alias("unique_merchants"),
                countDistinct("merchant_category").alias("unique_categories"),
                countDistinct("location_city").alias("cities_visited"),
                avg("fraud_score").alias("avg_fraud_score"),
                max("fraud_score").alias("max_fraud_score"),
                sum(when(col("risk_category") == "HIGH_RISK", 1).otherwise(0)).alias("high_risk_count"),
                collect_set("merchant_category").alias("merchant_categories_list")
            ) \
            .withColumn("customer_risk_profile",
                       when(col("high_risk_count") > col("daily_transaction_count") * 0.5, "HIGH_RISK_CUSTOMER")
                       .when(col("max_fraud_score") > 0.7, "ELEVATED_RISK_CUSTOMER")
                       .when(col("daily_total_amount") > 100000, "HIGH_VALUE_CUSTOMER")
                       .otherwise("STANDARD_CUSTOMER")) \
            .withColumn("geographic_mobility",
                       when(col("cities_visited") > 5, "HIGH_MOBILITY")
                       .when(col("cities_visited") > 2, "MODERATE_MOBILITY")
                       .otherwise("LOW_MOBILITY")) \
            .withColumn("merchant_diversity_score", col("unique_categories") / 20.0) \
            .withColumn("processing_timestamp", current_timestamp())
        
        # Merchant aggregations
        merchant_daily = daily_transactions \
            .groupBy("merchant_id", "merchant_category", "_silver_date") \
            .agg(
                count("*").alias("daily_transaction_volume"),
                sum("transaction_amount_usd").alias("daily_revenue"),
                avg("transaction_amount_usd").alias("avg_transaction_size"),
                countDistinct("customer_id").alias("unique_customers"),
                avg("fraud_score").alias("avg_fraud_score"),
                sum(when(col("risk_category") == "HIGH_RISK", 1).otherwise(0)).alias("high_risk_transactions"),
                countDistinct("location_city").alias("geographic_coverage")
            ) \
            .withColumn("merchant_risk_level",
                       when(col("avg_fraud_score") > 0.6, "HIGH_RISK_MERCHANT")
                       .when(col("high_risk_transactions") > col("daily_transaction_volume") * 0.3, "ELEVATED_RISK_MERCHANT")
                       .otherwise("STANDARD_MERCHANT")) \
            .withColumn("revenue_category",
                       when(col("daily_revenue") > 1000000, "ENTERPRISE")
                       .when(col("daily_revenue") > 100000, "LARGE")
                       .when(col("daily_revenue") > 10000, "MEDIUM")
                       .otherwise("SMALL")) \
            .withColumn("processing_timestamp", current_timestamp())
        
        # Write aggregations using merge for idempotent processing
        self._merge_daily_aggregations(customer_daily, f"{target_table}_customer_daily")
        self._merge_daily_aggregations(merchant_daily, f"{target_table}_merchant_daily")
        
        # Create cross-dimensional analytics
        self._create_cross_dimensional_analytics(daily_transactions, target_table, processing_date)
    
    def _merge_daily_aggregations(self, aggregated_df: DataFrame, target_table: str):
        """Merge daily aggregations with upsert logic"""
        
        try:
            target_delta = DeltaTable.forName(self.spark, target_table)
            
            # Merge based on natural keys
            if "customer_id" in aggregated_df.columns:
                merge_condition = "target.customer_id = source.customer_id AND target._silver_date = source._silver_date"
            else:
                merge_condition = "target.merchant_id = source.merchant_id AND target._silver_date = source._silver_date"
            
            target_delta.alias("target") \
                .merge(aggregated_df.alias("source"), merge_condition) \
                .whenMatchedUpdateAll() \
                .whenNotMatchedInsertAll() \
                .execute()
                
        except Exception:
            # Table doesn't exist, create it
            aggregated_df.write \
                .format("delta") \
                .mode("overwrite") \
                .partitionBy("_silver_date") \
                .saveAsTable(target_table)
    
    def _create_cross_dimensional_analytics(self, 
                                          transactions_df: DataFrame, 
                                          target_table: str, 
                                          processing_date: str):
        """Create cross-dimensional analytics cubes"""
        
        # Time-based analytics
        hourly_patterns = transactions_df \
            .withColumn("transaction_hour", hour(col("transaction_date"))) \
            .groupBy("transaction_hour", "merchant_category", "risk_category") \
            .agg(
                count("*").alias("transaction_count"),
                sum("transaction_amount_usd").alias("total_amount"),
                avg("transaction_amount_usd").alias("avg_amount"),
                avg("fraud_score").alias("avg_fraud_score")
            ) \
            .withColumn("processing_date", lit(processing_date)) \
            .withColumn("processing_timestamp", current_timestamp())
        
        # Geographic analytics
        geographic_patterns = transactions_df \
            .groupBy("location_country", "location_city", "merchant_category") \
            .agg(
                count("*").alias("transaction_count"),
                sum("transaction_amount_usd").alias("total_amount"),
                countDistinct("customer_id").alias("unique_customers"),
                countDistinct("merchant_id").alias("unique_merchants"),
                avg("fraud_score").alias("avg_fraud_score")
            ) \
            .withColumn("processing_date", lit(processing_date)) \
            .withColumn("processing_timestamp", current_timestamp())
        
        # Risk correlation analytics
        risk_correlations = transactions_df \
            .withColumn("amount_bucket",
                       when(col("transaction_amount_usd") > 50000, "VERY_HIGH")
                       .when(col("transaction_amount_usd") > 10000, "HIGH")
                       .when(col("transaction_amount_usd") > 1000, "MEDIUM")
                       .when(col("transaction_amount_usd") > 100, "LOW")
                       .otherwise("VERY_LOW")) \
            .groupBy("amount_bucket", "merchant_category", "risk_category") \
            .agg(
                count("*").alias("transaction_count"),
                avg("fraud_score").alias("avg_fraud_score"),
                stddev("fraud_score").alias("fraud_score_stddev"),
                countDistinct("customer_id").alias("unique_customers")
            ) \
            .withColumn("processing_date", lit(processing_date)) \
            .withColumn("processing_timestamp", current_timestamp())
        
        # Write analytics cubes
        self._write_analytics_cube(hourly_patterns, f"{target_table}_hourly_patterns")
        self._write_analytics_cube(geographic_patterns, f"{target_table}_geographic_patterns")
        self._write_analytics_cube(risk_correlations, f"{target_table}_risk_correlations")
    
    def _write_analytics_cube(self, cube_df: DataFrame, table_name: str):
        """Write analytics cube with optimization"""
        
        cube_df.write \
            .format("delta") \
            .mode("append") \
            .option("mergeSchema", "true") \
            .partitionBy("processing_date") \
            .saveAsTable(table_name)
    
    def optimize_delta_tables(self, table_patterns: List[str]):
        """Optimize Delta tables for performance"""
        
        for pattern in table_patterns:
            tables = self.spark.sql(f"SHOW TABLES LIKE '{pattern}'").collect()
            
            for table_row in tables:
                table_name = f"{table_row['database']}.{table_row['tableName']}"
                
                try:
                    # Optimize table
                    self.spark.sql(f"OPTIMIZE {table_name}")
                    
                    # Z-ORDER by commonly queried columns
                    if "customer" in table_name:
                        self.spark.sql(f"OPTIMIZE {table_name} ZORDER BY (customer_id, _silver_date)")
                    elif "merchant" in table_name:
                        self.spark.sql(f"OPTIMIZE {table_name} ZORDER BY (merchant_id, merchant_category)")
                    elif "risk" in table_name:
                        self.spark.sql(f"OPTIMIZE {table_name} ZORDER BY (risk_category, processing_date)")
                    
                    print(f"Optimized table: {table_name}")
                    
                except Exception as e:
                    print(f"Failed to optimize {table_name}: {str(e)}")
    
    def vacuum_old_data(self, table_patterns: List[str], retention_hours: int = 168):
        """Vacuum old data files (default 7 days)"""
        
        for pattern in table_patterns:
            tables = self.spark.sql(f"SHOW TABLES LIKE '{pattern}'").collect()
            
            for table_row in tables:
                table_name = f"{table_row['database']}.{table_row['tableName']}"
                
                try:
                    self.spark.sql(f"VACUUM {table_name} RETAIN {retention_hours} HOURS")
                    print(f"Vacuumed table: {table_name}")
                    
                except Exception as e:
                    print(f"Failed to vacuum {table_name}: {str(e)}")
    
    def generate_data_quality_report(self, tables: List[str]) -> DataFrame:
        """Generate comprehensive data quality report"""
        
        quality_metrics = []
        
        for table in tables:
            try:
                df = self.spark.read.format("delta").table(table)
                row_count = df.count()
                
                if row_count > 0:
                    # Calculate quality metrics
                    null_counts = {}
                    for column in df.columns:
                        if not column.startswith("_"):  # Skip metadata columns
                            null_count = df.filter(col(column).isNull()).count()
                            null_counts[column] = null_count
                    
                    # Overall completeness
                    total_cells = row_count * len([c for c in df.columns if not c.startswith("_")])
                    total_nulls = sum(null_counts.values())
                    completeness = (total_cells - total_nulls) / total_cells if total_cells > 0 else 0
                    
                    quality_metrics.append({
                        "table_name": table,
                        "row_count": row_count,
                        "column_count": len(df.columns),
                        "completeness_score": completeness,
                        "null_columns": [k for k, v in null_counts.items() if v > 0],
                        "most_null_column": max(null_counts.items(), key=lambda x: x[1])[0] if null_counts else None,
                        "assessment_timestamp": current_timestamp()
                    })
                
            except Exception as e:
                quality_metrics.append({
                    "table_name": table,
                    "row_count": 0,
                    "column_count": 0,
                    "completeness_score": 0.0,
                    "null_columns": [],
                    "most_null_column": None,
                    "error_message": str(e),
                    "assessment_timestamp": current_timestamp()
                })
        
        return self.spark.createDataFrame(quality_metrics)

# Usage Example
if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("DeltaLakeBatchProcessor") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    processor = DeltaLakeBatchProcessor(spark)
    
    # Process daily aggregations
    processor.create_daily_aggregations(
        "silver.financial_transactions",
        "gold.daily_analytics",
        "2024-01-15"
    )
    
    # Optimize tables
    processor.optimize_delta_tables([
        "gold.daily_analytics_*",
        "silver.financial_*",
        "bronze.*"
    ])
    
    # Generate quality report
    quality_report = processor.generate_data_quality_report([
        "bronze.financial_transactions",
        "silver.financial_transactions",
        "gold.daily_analytics_customer_daily"
    ])
    
    quality_report.show(truncate=False)
```

## ðŸ¤– MLflow Model Management Integration

### End-to-End ML Pipeline

```python
import mlflow
import mlflow.sklearn
import mlflow.databricks
from mlflow.tracking import MlflowClient
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer
from pyspark.ml.classification import RandomForestClassifier, GBTClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
import numpy as np
from typing import Dict, Any, List, Tuple

class MLflowModelManager:
    def __init__(self, spark: SparkSession, experiment_name: str):
        self.spark = spark
        self.client = MlflowClient()
        
        # Set up MLflow experiment
        try:
            experiment_id = self.client.create_experiment(experiment_name)
        except Exception:
            experiment = self.client.get_experiment_by_name(experiment_name)
            experiment_id = experiment.experiment_id
        
        mlflow.set_experiment(experiment_name)
        self.experiment_id = experiment_id
    
    def prepare_fraud_detection_features(self, table_name: str) -> DataFrame:
        """Prepare features for fraud detection model"""
        
        # Read data from Gold layer
        base_df = self.spark.read.format("delta").table(table_name)
        
        # Feature engineering
        features_df = base_df \
            .withColumn("amount_log", log1p(col("transaction_amount_usd"))) \
            .withColumn("hour_of_day", hour(col("transaction_date"))) \
            .withColumn("day_of_week", dayofweek(col("transaction_date"))) \
            .withColumn("is_weekend", when(col("day_of_week").isin(1, 7), 1).otherwise(0)) \
            .withColumn("is_night_transaction", when(col("hour_of_day").between(0, 6), 1).otherwise(0)) \
            .withColumn("amount_to_balance_ratio", col("transaction_amount_usd") / col("account_balance")) \
            .withColumn("days_since_last_transaction", 
                       datediff(col("transaction_date"), 
                               lag(col("transaction_date")).over(
                                   Window.partitionBy("customer_id").orderBy("transaction_date")))) \
            .withColumn("transaction_velocity_1h", 
                       count("*").over(
                           Window.partitionBy("customer_id")
                                 .orderBy(col("transaction_date").cast("timestamp").cast("long"))
                                 .rangeBetween(-3600, 0))) \
            .withColumn("amount_velocity_1h",
                       sum("transaction_amount_usd").over(
                           Window.partitionBy("customer_id")
                                 .orderBy(col("transaction_date").cast("timestamp").cast("long"))
                                 .rangeBetween(-3600, 0))) \
            .withColumn("unique_merchants_1h",
                       countDistinct("merchant_id").over(
                           Window.partitionBy("customer_id")
                                 .orderBy(col("transaction_date").cast("timestamp").cast("long"))
                                 .rangeBetween(-3600, 0))) \
            .withColumn("fraud_label", when(col("fraud_score") > 0.7, 1).otherwise(0))
        
        return features_df
    
    def train_fraud_detection_model(self, 
                                  training_data: DataFrame,
                                  model_name: str = "fraud_detection_model") -> str:
        """Train fraud detection model with MLflow tracking"""
        
        with mlflow.start_run() as run:
            # Log parameters
            mlflow.log_param("model_type", "ensemble")
            mlflow.log_param("feature_count", len(training_data.columns) - 1)
            mlflow.log_param("training_rows", training_data.count())
            
            # Prepare features
            feature_cols = [
                "amount_log", "hour_of_day", "day_of_week", "is_weekend", 
                "is_night_transaction", "amount_to_balance_ratio",
                "transaction_velocity_1h", "amount_velocity_1h", "unique_merchants_1h"
            ]
            
            categorical_cols = ["merchant_category", "location_country"]
            
            # String indexing for categorical variables
            indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_idx", handleInvalid="keep") 
                       for col in categorical_cols]
            
            # Feature assembly
            assembler = VectorAssembler(
                inputCols=feature_cols + [f"{col}_idx" for col in categorical_cols],
                outputCol="features"
            )
            
            # Feature scaling
            scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
            
            # Model definition
            rf_classifier = RandomForestClassifier(
                featuresCol="scaled_features",
                labelCol="fraud_label",
                numTrees=100,
                maxDepth=10,
                seed=42
            )
            
            gbt_classifier = GBTClassifier(
                featuresCol="scaled_features",
                labelCol="fraud_label",
                maxIter=100,
                maxDepth=8,
                seed=42
            )
            
            # Pipeline
            pipeline = Pipeline(stages=indexers + [assembler, scaler, rf_classifier])
            
            # Train-test split
            train_df, test_df = training_data.randomSplit([0.8, 0.2], seed=42)
            
            # Cross-validation
            evaluator = BinaryClassificationEvaluator(
                labelCol="fraud_label",
                rawPredictionCol="rawPrediction",
                metricName="areaUnderROC"
            )
            
            param_grid = ParamGridBuilder() \
                .addGrid(rf_classifier.numTrees, [80, 100, 120]) \
                .addGrid(rf_classifier.maxDepth, [8, 10, 12]) \
                .build()
            
            cv = CrossValidator(
                estimator=pipeline,
                estimatorParamMaps=param_grid,
                evaluator=evaluator,
                numFolds=3,
                seed=42
            )
            
            # Train model
            cv_model = cv.fit(train_df)
            best_model = cv_model.bestModel
            
            # Evaluate on test set
            test_predictions = best_model.transform(test_df)
            
            # Calculate metrics
            auc_score = evaluator.evaluate(test_predictions)
            
            # Additional metrics
            multiclass_evaluator = MulticlassClassificationEvaluator(
                labelCol="fraud_label",
                predictionCol="prediction"
            )
            
            precision = multiclass_evaluator.evaluate(test_predictions, {multiclass_evaluator.metricName: "weightedPrecision"})
            recall = multiclass_evaluator.evaluate(test_predictions, {multiclass_evaluator.metricName: "weightedRecall"})
            f1_score = multiclass_evaluator.evaluate(test_predictions, {multiclass_evaluator.metricName: "f1"})
            
            # Log metrics
            mlflow.log_metric("auc_score", auc_score)
            mlflow.log_metric("precision", precision)
            mlflow.log_metric("recall", recall)
            mlflow.log_metric("f1_score", f1_score)
            
            # Log model
            mlflow.spark.log_model(
                best_model,
                "model",
                registered_model_name=model_name
            )
            
            # Feature importance analysis
            rf_model = best_model.stages[-1]
            if hasattr(rf_model, 'featureImportances'):
                feature_importance = {
                    f"feature_{i}": float(importance) 
                    for i, importance in enumerate(rf_model.featureImportances.toArray())
                }
                mlflow.log_dict(feature_importance, "feature_importance.json")
            
            print(f"Model training completed. AUC: {auc_score:.4f}")
            return run.info.run_id
    
    def deploy_model_for_inference(self, 
                                  model_name: str, 
                                  model_version: str,
                                  endpoint_name: str) -> Dict[str, Any]:
        """Deploy model for real-time inference"""
        
        # Register model version
        model_version_details = self.client.create_model_version(
            name=model_name,
            source=f"runs:/{model_version}/model",
            description="Fraud detection model for real-time inference"
        )
        
        # Transition to production
        self.client.transition_model_version_stage(
            name=model_name,
            version=model_version_details.version,
            stage="Production"
        )
        
        # Create inference configuration
        inference_config = {
            "model_name": model_name,
            "model_version": model_version_details.version,
            "endpoint_name": endpoint_name,
            "compute_config": {
                "min_capacity": 1,
                "max_capacity": 10,
                "target_capacity": 2
            },
            "environment_vars": {
                "MLFLOW_TRACKING_URI": mlflow.get_tracking_uri(),
                "MODEL_NAME": model_name
            }
        }
        
        return inference_config
    
    def batch_inference_pipeline(self, 
                                model_name: str,
                                input_table: str,
                                output_table: str,
                                model_stage: str = "Production") -> None:
        """Run batch inference on new data"""
        
        # Load production model
        model_uri = f"models:/{model_name}/{model_stage}"
        model = mlflow.spark.load_model(model_uri)
        
        # Read new data
        new_data = self.spark.read.format("delta").table(input_table)
        
        # Prepare features (same as training)
        features_df = self.prepare_fraud_detection_features_for_inference(new_data)
        
        # Make predictions
        predictions = model.transform(features_df)
        
        # Add inference metadata
        enriched_predictions = predictions \
            .withColumn("model_name", lit(model_name)) \
            .withColumn("model_stage", lit(model_stage)) \
            .withColumn("inference_timestamp", current_timestamp()) \
            .withColumn("prediction_confidence", 
                       when(col("probability").getItem(1) > 0.8, "HIGH")
                       .when(col("probability").getItem(1) > 0.6, "MEDIUM")
                       .otherwise("LOW")) \
            .withColumn("requires_review", 
                       when(col("prediction") == 1, True).otherwise(False))
        
        # Write predictions
        enriched_predictions \
            .write \
            .format("delta") \
            .mode("append") \
            .option("mergeSchema", "true") \
            .partitionBy("inference_timestamp") \
            .saveAsTable(output_table)
        
        # Log inference metrics
        fraud_predictions = enriched_predictions.filter(col("prediction") == 1).count()
        total_predictions = enriched_predictions.count()
        fraud_rate = fraud_predictions / total_predictions if total_predictions > 0 else 0
        
        with mlflow.start_run():
            mlflow.log_metric("batch_size", total_predictions)
            mlflow.log_metric("fraud_predictions", fraud_predictions)
            mlflow.log_metric("fraud_rate", fraud_rate)
            mlflow.log_param("input_table", input_table)
            mlflow.log_param("output_table", output_table)
    
    def prepare_fraud_detection_features_for_inference(self, df: DataFrame) -> DataFrame:
        """Prepare features for inference (same logic as training)"""
        # This should match the feature engineering in prepare_fraud_detection_features
        # but without the fraud_label column
        
        return df \
            .withColumn("amount_log", log1p(col("transaction_amount_usd"))) \
            .withColumn("hour_of_day", hour(col("transaction_date"))) \
            .withColumn("day_of_week", dayofweek(col("transaction_date"))) \
            .withColumn("is_weekend", when(col("day_of_week").isin(1, 7), 1).otherwise(0)) \
            .withColumn("is_night_transaction", when(col("hour_of_day").between(0, 6), 1).otherwise(0)) \
            .withColumn("amount_to_balance_ratio", col("transaction_amount_usd") / col("account_balance")) \
            .withColumn("transaction_velocity_1h", 
                       count("*").over(
                           Window.partitionBy("customer_id")
                                 .orderBy(col("transaction_date").cast("timestamp").cast("long"))
                                 .rangeBetween(-3600, 0))) \
            .withColumn("amount_velocity_1h",
                       sum("transaction_amount_usd").over(
                           Window.partitionBy("customer_id")
                                 .orderBy(col("transaction_date").cast("timestamp").cast("long"))
                                 .rangeBetween(-3600, 0))) \
            .withColumn("unique_merchants_1h",
                       countDistinct("merchant_id").over(
                           Window.partitionBy("customer_id")
                                 .orderBy(col("transaction_date").cast("timestamp").cast("long"))
                                 .rangeBetween(-3600, 0)))
    
    def monitor_model_performance(self, model_name: str, predictions_table: str) -> Dict[str, Any]:
        """Monitor model performance and detect drift"""
        
        # Read recent predictions
        recent_predictions = self.spark.read.format("delta").table(predictions_table) \
            .filter(col("inference_timestamp") >= date_sub(current_date(), 7))
        
        # Calculate performance metrics
        total_predictions = recent_predictions.count()
        high_confidence_predictions = recent_predictions.filter(col("prediction_confidence") == "HIGH").count()
        fraud_predictions = recent_predictions.filter(col("prediction") == 1).count()
        
        # Detect potential drift
        fraud_rate = fraud_predictions / total_predictions if total_predictions > 0 else 0
        confidence_rate = high_confidence_predictions / total_predictions if total_predictions > 0 else 0
        
        # Calculate feature drift (simplified)
        feature_stats = recent_predictions.agg(
            avg("amount_log").alias("avg_amount_log"),
            stddev("amount_log").alias("stddev_amount_log"),
            avg("transaction_velocity_1h").alias("avg_velocity"),
            stddev("transaction_velocity_1h").alias("stddev_velocity")
        ).collect()[0]
        
        monitoring_results = {
            "model_name": model_name,
            "monitoring_period": "7_days",
            "total_predictions": total_predictions,
            "fraud_rate": fraud_rate,
            "confidence_rate": confidence_rate,
            "feature_statistics": feature_stats.asDict(),
            "potential_drift": fraud_rate > 0.1 or confidence_rate < 0.6,  # Simple thresholds
            "monitoring_timestamp": current_timestamp()
        }
        
        # Log monitoring metrics to MLflow
        with mlflow.start_run():
            mlflow.log_metrics({
                "fraud_rate": fraud_rate,
                "confidence_rate": confidence_rate,
                "total_predictions": float(total_predictions)
            })
            mlflow.log_dict(monitoring_results, "monitoring_report.json")
        
        return monitoring_results

# Usage Example
if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("MLflowModelManager") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    # Initialize MLflow manager
    ml_manager = MLflowModelManager(spark, "fraud_detection_experiment")
    
    # Prepare training data
    training_data = ml_manager.prepare_fraud_detection_features("gold.customer_behavior")
    
    # Train model
    run_id = ml_manager.train_fraud_detection_model(training_data)
    
    # Deploy model
    deployment_config = ml_manager.deploy_model_for_inference(
        "fraud_detection_model",
        run_id,
        "fraud_detection_endpoint"
    )
    
    print(f"Model deployed: {deployment_config}")
    
    # Run batch inference
    ml_manager.batch_inference_pipeline(
        "fraud_detection_model",
        "silver.financial_transactions",
        "gold.fraud_predictions"
    )
    
    # Monitor performance
    monitoring_report = ml_manager.monitor_model_performance(
        "fraud_detection_model",
        "gold.fraud_predictions"
    )
    
    print(f"Monitoring report: {monitoring_report}")
```

## ðŸ”§ Integration Configuration

### Complete Azure Databricks Setup

```yaml
# databricks-workspace-config.yml
databricks:
  workspace:
    name: "fintech-big-data-platform"
    region: "East US 2"
    pricing_tier: "premium"
    
  clusters:
    streaming_cluster:
      name: "real-time-streaming"
      node_type: "Standard_D13_v2"
      min_workers: 2
      max_workers: 8
      auto_termination_minutes: 120
      spark_config:
        "spark.sql.adaptive.enabled": "true"
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
        "spark.databricks.delta.optimizeWrite.enabled": "true"
        "spark.databricks.delta.autoCompact.enabled": "true"
        "spark.sql.streaming.metricsEnabled": "true"
      
    batch_cluster:
      name: "batch-processing"
      node_type: "Standard_D14_v2"
      min_workers: 4
      max_workers: 16
      auto_termination_minutes: 60
      spark_config:
        "spark.sql.adaptive.enabled": "true"
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
        "spark.databricks.delta.optimizeWrite.enabled": "true"
        "spark.databricks.delta.autoCompact.enabled": "true"
        
    ml_cluster:
      name: "ml-training"
      node_type: "Standard_D15_v2"
      min_workers: 2
      max_workers: 12
      auto_termination_minutes: 180
      spark_config:
        "spark.sql.adaptive.enabled": "true"
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
      libraries:
        - "mlflow"
        - "scikit-learn"
        - "xgboost"
        - "hyperopt"

  storage:
    delta_lake:
      bronze_path: "/mnt/data/bronze"
      silver_path: "/mnt/data/silver"
      gold_path: "/mnt/data/gold"
      feature_store_path: "/mnt/data/feature_store"
      
  secrets:
    kafka_config:
      bootstrap_servers: "{{secrets/kafka/bootstrap_servers}}"
      security_protocol: "SASL_SSL"
      sasl_mechanism: "PLAIN"
      sasl_username: "{{secrets/kafka/username}}"
      sasl_password: "{{secrets/kafka/password}}"
      
  jobs:
    real_time_processing:
      name: "Real-Time Transaction Processing"
      cluster: "real-time-streaming"
      schedule: "continuous"
      notebook_path: "/notebooks/real_time_processor"
      
    batch_daily_aggregations:
      name: "Daily Batch Aggregations"
      cluster: "batch-processing"
      schedule: "0 2 * * *"  # Daily at 2 AM
      notebook_path: "/notebooks/batch_aggregations"
      
    ml_model_training:
      name: "ML Model Training Pipeline"
      cluster: "ml-training"
      schedule: "0 6 * * 0"  # Weekly on Sunday at 6 AM
      notebook_path: "/notebooks/ml_training_pipeline"
      
    model_inference:
      name: "Batch Model Inference"
      cluster: "batch-processing"
      schedule: "0 */4 * * *"  # Every 4 hours
      notebook_path: "/notebooks/batch_inference"
```

This comprehensive Azure Databricks integration provides:

1. **Unified Processing Platform**: Real-time, near-time, and batch processing with Delta Lake storage
2. **MLflow Integration**: Complete model lifecycle management with automated deployment
3. **Performance Optimization**: Auto-scaling clusters with optimized Spark configurations
4. **Data Quality Integration**: Seamless integration with our governance and quality frameworks
5. **Monitoring and Alerting**: Built-in performance monitoring and drift detection
6. **Scalable Architecture**: Auto-scaling based on workload with cost optimization
7. **Enterprise Security**: Integration with Azure AD and role-based access control

The platform ensures accurate data flow through automated quality gates while providing the scalability needed for enterprise-grade financial AI inference workloads.